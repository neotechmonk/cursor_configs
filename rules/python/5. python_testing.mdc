---
title: Python Testing Standards
description: Guidelines for unit tests, mocks, structure, execution, documentation, and coverage.
appliesTo:
  - python
---

# Framework
- Use **pytest**.
- Run with **uv**: `uv run pytest …`.

# Folder structure
- All tests live under `tests/`.
- Types
  - Unit `tests/module/unit/sub_module?/test_xx.py`
  - Integration `tests/module/integration/test_xx.py`
  - System `tests/system/test_xx.py`
- Mirror `src/` structure.
- No `__init__.py` unless cross-importing helpers.
- Common dirs:
  - `tests/mocks/[module]`
  - `tests/data/[module]/valid/`
  - `tests/data/[module]/invalid/*`
  
## Markers
- Add to `pyproject.toml` (preferable) or `pytest.ini`:

```toml
[tool.pytest.ini_options]
markers = [
    "unit: fast, isolated tests (no filesystem/network)",
    "integration: integration tests with real wiring (minimal I/O)",
    "system: end-to-end/smoke/performance tests"
]
```

# Test Types
Scope definitions (crisp + enforceable)

## Unit (fast, pure, tiny)
Purpose: Prove a function/class behaves correctly in isolation.
- I/O: none (no disk, no net, no sleep); use fakes/in-mem doubles
- Dependencies: mocked/stubbed to interfaces (protocols)
- Data size: minimal; use inlined literals or tiny fixtures
- Runtime: sub-second per test; whole unit suite ≤2–3 min
- Common targets (your codebase):
  - SessionTransformer logic
  - Cache policies (MemoryCache.get/put/invalidate)
  - Helpers: validators, parsers, mappers (e.g., YAML→model field mapping)
- Eg
  ```python
  @pytest.mark.unit
  def test_transformer_maps_symbol_fields():
      raw = {"symbols": [{"symbol": "AAPL", "timeframe": "1m"}]}
      out = SessionTransformer()(raw)
      assert out.symbols[0].symbol == "AAPL"
  ```

## Integration (real wiring, minimal externalities)

Purpose: Prove modules work together (adapters+transformers+services) using real objects and tiny deterministic inputs.
- I/O: allowed if local & tiny (e.g., read a small YAML under tests/data)
- Dependencies: real internal components; mock only true externals (network, clock if nondeterministic)
- Data size: tiny "golden" files (e.g., alpha.yaml)
- Runtime: seconds per test; integration pack ≤5–8 min
- Common targets (your codebase):
  - TradingSessionService.get() reads YAML via YamlSessionAdapter, transforms, builds session
  - Equivalence: dict-adapter vs YAML-adapter produce same resolved model
  - Cache read-through semantics (first build vs subsequent get)
- E.g.
  ```python
  @pytest.mark.integration
  def test_session_get_builds_from_yaml(tmp_path):
      (tmp_path/"alpha.yaml").write_text(
          "name: alpha\nproviders:{data:stub,execution:stub}\n"
          "symbols:[{symbol:AAPL,timeframe:'1m'}]\n"
      )
      svc = TradingSessionService(sessions_dir=tmp_path, ...)
      s = svc.get("alpha")
      assert s.name == "alpha"
      assert s.symbols
  ```

## System (E2E smoke, resilience, perf)

Purpose: Prove critical user journeys work across the app boundary.
- I/O: yes (orchestration-level), but controlled and fast; external deps stubbed at boundary if needed
- Dependencies: "real enough" stack with boundary doubles (e.g., provider stub HTTP server or in-proc fake)
- Data size: representative but small (e.g., 10k bars perf cap)
- Runtime: kept short; smoke ≤1–2 min, perf capped & tagged @slow
- Common targets (your codebase):
  - Boot a session → process a few ticks → emit order intent → update portfolio → clean shutdown
  - Resilience: provider restart; idempotent shutdown; recovery of state
  - Guardrail perf: "process N bars under T sec" on CI boxes
- Example (smoke):
  ```python
  @pytest.mark.system
  def test_end_to_end_smoke(fake_provider, fake_execution):
      s = boot_minimal_session(provider=fake_provider, execution=fake_execution)
      s.tick()  # drives strategy
      assert fake_execution.last_intent is not None
      s.shutdown()
  ```

## Test Types : Quick decision tree
- Does it touch filesystem/network/time/process env?
- No → Unit
- Yes, but only within your app's modules (real wiring, tiny files, external I/O mocked) → Integration
- Yes, crosses multiple modules/endpoints and exercises a user workflow end-to-end → System

# Writing Tests
- Prefer **plain functions** (`def test_...`).
  - Use `class Test...:` only if grouping with shared fixtures/marks.
  - Split themes if needed: `test_module_func1.py`, `test_module_func2.py`.
- Start with **happy path → edge cases → error conditions**.
- Type hint tests.
  - If a concrete implementation adheres to a contract, use a **Protocol** for typing.
- Scope:
  - **Unit:** isolate functions/classes, mock dependencies.
  - **Integration:** DB/API workflows.
  - **E2E:** only critical paths.
  
## Test Naming Conventions

### Pattern: `test_[method]_[scenario]_[expected_result]`
- **Method**: The function/method being tested
- **Scenario**: The specific condition or input
- **Expected Result**: What should happen

**Good Examples:**
```python
def test_calculate_tax_with_standard_rate_returns_correct_amount()
def test_validate_email_with_invalid_format_raises_exception()
def test_process_order_with_insufficient_inventory_returns_error()
def test_get_user_by_id_with_nonexistent_id_returns_none()
```

**Bad Examples:**
```python
def test_calculator()  # Too vague
def test_add()  # Missing scenario
def test_works()  # Doesn't describe behavior
def test_calculate_tax_1()  # Meaningless number
```

# Mocks
- Centralize in `tests/mocks/[module]/`.
- Mock to `interface/protocol/abstract class` where possible and not concrete implementations.
- Use `pytest-mock`(preferable) or `unittest.mock` with `autospec` where possible

## Mocking Strategies

### Basic Mocking
```python
# Mock external dependencies
@pytest.fixture
def mock_database():
    with patch('module.database') as mock_db:
        mock_db.get_user.return_value = User(id=1, name="Test")
        yield mock_db

def test_user_service_gets_user_from_database(mock_database):
    service = UserService(mock_database)
    user = service.get_user(1)
    assert user.name == "Test"
    mock_database.get_user.assert_called_once_with(1)
```

### Advanced Mocking Patterns

#### 1. Context Manager Mocking
```python
def test_file_processing():
    with patch('builtins.open', mock_open(read_data='test data')) as mock_file:
        result = process_file('test.txt')
        mock_file.assert_called_once_with('test.txt', 'r')
        assert result == 'processed: test data'
```

#### 2. Side Effects and Exceptions
```python
def test_handles_database_error():
    mock_db = MagicMock()
    mock_db.get_user.side_effect = DatabaseError("Connection failed")
    
    with pytest.raises(ServiceError, match="Database unavailable"):
        service.get_user(1)
```

#### 3. Async Mocking
```python
@pytest.fixture
async def mock_async_client():
    mock_client = AsyncMock()
    mock_client.fetch_data.return_value = {"status": "success"}
    return mock_client

@pytest.mark.asyncio
async def test_async_service(mock_async_client):
    service = AsyncService(mock_async_client)
    result = await service.process_data()
    assert result["status"] == "success"
```

#### 4. Property Mocking
```python
def test_configuration_property():
    with patch.object(Config, 'api_key', new_callable=PropertyMock) as mock_key:
        mock_key.return_value = 'test-key'
        config = Config()
        assert config.api_key == 'test-key'
```

#### 5. Partial Mocking
```python
def test_partial_mock():
    with patch.object(ExternalService, 'expensive_operation') as mock_expensive:
        mock_expensive.return_value = 'cached_result'
        
        service = Service()
        result = service.process_with_cache()
        assert result == 'cached_result'
```

#### 6. Mock Chaining
```python
def test_mock_chaining():
    mock_client = MagicMock()
    mock_client.api.users.get.return_value = {"id": 1, "name": "Test"}
    
    service = APIService(mock_client)
    user = service.get_user(1)
    assert user["name"] == "Test"
```

#### 7. Call Tracking
```python
def test_call_tracking():
    mock_logger = MagicMock()
    
    with patch('module.logger', mock_logger):
        service = Service()
        service.process_item("test")
        
        # Verify specific calls
        mock_logger.info.assert_called_with("Processing item: test")
        assert mock_logger.info.call_count == 1
```

#### 8. Mock Validation
```python
def test_mock_validation():
    mock_validator = MagicMock()
    mock_validator.validate.return_value = True
    
    service = Service(validator=mock_validator)
    result = service.process_data("valid_data")
    
    # Verify mock was called correctly
    mock_validator.validate.assert_called_once_with("valid_data")
    assert result is not None
```

### Mock Best Practices
- Use `spec` or `autospec` to catch interface changes
- Mock at the boundary of your system
- Prefer dependency injection over patching
- Use `side_effect` for complex behaviors
- Verify mock interactions with `assert_called_with`
- Clean up mocks in teardown if needed

### Mock Anti Pattern
- The class or function under test
- Simple data structures (lists, dicts, primitives)
- Pure functions (mathematical operations, validators)
- Value objects (immutable data containers)

# Execution
- Iterate locally with narrow scope:
  - `uv run pytest tests/[module]/test_file.py::test_func -v`
- Use markers (`@pytest.mark.skip`) for staged rollout.
- Coverage with:
  - `uv run pytest tests/[module]/ --cov=src/[module] --cov-report=term-missing`
- Use parameterization (`@pytest.mark.parametrize`) to reduce duplication.

<!-- # Documentation -->
<!-- - **Planning:**   -->
<!--   - Open `.cursor/templates/test_run.md`, fill in PLAN, paste into `tests/[module]/test.md`. -->
<!-- - **During runs:**   -->
<!--   - Tick CHECKLIST items in `test.md` as tests pass.   -->
<!--   - Keep iterations **append-only** (don't overwrite previous runs). -->
<!-- - **After runs:**   -->
<!--   - Summarize results under SUMMARY in `test.md`.   -->
<!--   - Remove iteration blocks + checklists once complete.   -->
<!--   - Keep a single **living note per module** (`tests/[module]/test.md`). -->

# Anti-patterns to Avoid
- Unit tests that:
  - Write files or use sleep()
  - Have excessive mocking
  - Mix concerns across test levels
- Integration tests that:
  - Mock the module under test
  - Have unclear boundaries
- System tests that:
  - Assert on implementation details
  - Test too many internals
- Fixtures that:
  - Create complex dependency graphs
  - Perform I/O in unit test scope
  - Hide important setup in conftest.py

# Time Budget Guidelines
- Unit Tests:
  - Individual: < 10ms per test
  - Suite Total: ≤ 2-3 minutes
- Integration Tests:  
  - Individual: ≤ 1-3 seconds
  - Suite Total: ≤ 5-8 minutes
- System Tests:
  - Smoke Suite: ≤ 60-120 seconds total
- Slow Tests:
  - Mark with @pytest.mark.slow
  - Exclude from PR validation runs

# After Passing
- Run linter: `uv run ruff check --fix`.
- Report:
  - Coverage gaps,
  - Fixture overuse,
  - Excessive patching,
  - Test naming clarity vs. `src` responsibilities.